import uvicorn
from fastapi.middleware.cors import CORSMiddleware
from functools import partial
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from fastapi import FastAPI, HTTPException, Depends, Security
from pydantic import BaseModel
from enum import Enum
import requests
from typing import List
from fastapi.staticfiles import StaticFiles
from fastapi.security import APIKeyHeader
from src.utils.rate_limiter import rate_limiter
from src.tts import text_to_speech, text_to_speech_v2
from src.tts.prompts import tts_prompt_template
from src.ocr import extract, get_image_size
from src.config import MAX_WORKERS, GOOGLE_APPLICATION_CREDENTIALS

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = GOOGLE_APPLICATION_CREDENTIALS


class OcrRequest(BaseModel):
    image_url: str
    id: str
    use_ai: bool = False


class TypeBubble(Enum):
    DIALOGUE = "dialogue"
    THOUGHT = "thought"
    NARRATION = "narration"
    SOUND_EFFECT = "sound_effect"
    BACKGROUND = "background"


class OcrItem(BaseModel):
    min_y: int
    min_x: int
    max_y: int
    max_x: int
    text: str
    panel_id: str
    type: TypeBubble
    model_used: str


class OcrResponse(BaseModel):
    id: str
    items: list[OcrItem]
    path_audio: str = ""
    has_bubble: bool = False


app = FastAPI()

# C·∫•u h√¨nh ph·ª•c v·ª• file tƒ©nh t·ª´ th∆∞ m·ª•c public
app.mount("/public", StaticFiles(directory="public"), name="public")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

API_KEY = os.getenv("API_KEY")
API_KEY_NAME = "X-API-KEY"
GEMINI_API_KEY = os.getenv("GOOGLE_AI_API_KEY")

api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

if API_KEY is None:
    API_KEY = "kimthi123123"


async def get_api_key(api_key: str = Security(api_key_header)):
    if api_key == API_KEY:
        return api_key
    raise HTTPException(status_code=401, detail="Invalid API key")


def has_valid_text(ocr_items: List[OcrItem]) -> bool:
    """
    Ki·ªÉm tra xem c√≥ text h·ª£p l·ªá ƒë·ªÉ t·∫°o TTS hay kh√¥ng

    Args:
        ocr_items: Danh s√°ch OCR items

    Returns:
        bool: True n·∫øu c√≥ text h·ª£p l·ªá, False n·∫øu kh√¥ng
    """
    if not ocr_items:
        return False

    for item in ocr_items:
        if not item.text:
            continue

        text = item.text.strip()

        if not text:
            continue

        if text in ["[]", "{}", "null", "None"]:
            continue

        if all(c in " \n\t\r.,!?;:-()[]{}\"'" for c in text):
            continue

        if len(text) < 2:
            continue

        return True

    return False


@app.post("/multi-image-ocr", response_model=List[OcrResponse], dependencies=[Depends(get_api_key)])
async def multi_image_ocr(images: list[OcrRequest]):
    """
    API tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ nhi·ªÅu ·∫£nh comic s·ª≠ d·ª•ng ƒëa lu·ªìng (b·ªè chunking)

    Args:
        request: Object ch·ª©a danh s√°ch c√°c ·∫£nh c·∫ßn x·ª≠ l√Ω

    Returns:
        List[OcrResponse]: Danh s√°ch k·∫øt qu·∫£ OCR t·ª´ c√°c ·∫£nh
    """
    try:
        import time
        start_time = time.time()
        # H√†m x·ª≠ l√Ω 1 ·∫£nh an to√†n
        process_func = partial(process_single_image_safe)

        results = []

        # Kh·ªüi executor v·ªõi t·ªëi ƒëa MAX_WORKERS lu·ªìng
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Submit t·∫•t c·∫£ ·∫£nh c√πng l√∫c
            future_to_image = {
                executor.submit(process_func, img.image_url, img.id, img.use_ai): img
                for img in images
            }

            for future in as_completed(future_to_image):
                img = future_to_image[future]
                try:
                    res = future.result()  # OcrResponse cho ·∫£nh ƒë√≥
                    results.append(res)
                except Exception as e:
                    print(f"L·ªói x·ª≠ l√Ω ·∫£nh {img.id}: {e}")
                    results.append(OcrResponse(id=img.id, items=[]))

        # S·∫Øp x·∫øp k·∫øt qu·∫£ ƒë√∫ng theo th·ª© t·ª± input (TTS ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω trong thread)
        id_to_res = {r.id: r for r in results if r is not None}
        ordered = [id_to_res.get(img.id, OcrResponse(id=img.id, items=[], path_audio=""))
                   for img in images]

        end_time = time.time()
        print(f"Th·ªùi gian x·ª≠ l√Ω batch OCR: {end_time - start_time:.2f} gi√¢y")
        return ordered

    except Exception as e:
        print(f"L·ªói x·ª≠ l√Ω batch OCR: {e}")
        raise HTTPException(status_code=500, detail=f"L·ªói batch OCR: {e}")


def process_single_image_safe(image_url: str, id: str, use_ai: bool) -> OcrResponse:
    """
    H√†m wrapper an to√†n cho x·ª≠ l√Ω ƒëa lu·ªìng - bao g·ªìm c·∫£ OCR v√† TTS

    Args:
        image_url: URL c·ªßa ·∫£nh c·∫ßn x·ª≠ l√Ω
        id: ID c·ªßa ·∫£nh

    Returns:
        OcrResponse: K·∫øt qu·∫£ OCR v√† TTS ho·∫∑c None n·∫øu c√≥ l·ªói
    """
    try:
        # X·ª≠ l√Ω OCR tr∆∞·ªõc
        result = process_single_image(image_url, id)

        if use_ai:
            # T·∫°o TTS ngay sau khi OCR xong n·∫øu c√≥ items h·ª£p l·ªá
            if result.items and has_valid_text(result.items):
                try:
                    os.makedirs("public/tts", exist_ok=True)
                    prompt = create_tts_prompt(result.items)
                    path_audio = f"public/tts/{result.id}"
                    result.has_bubble = True
                    if GEMINI_API_KEY:
                        # Hi·ªÉn th·ªã th√¥ng tin rate limit tr∆∞·ªõc khi g·ªçi TTS
                        remaining_tts = rate_limiter.get_remaining_requests(
                            "gemini-2.5-flash-preview-tts")
                        print(f"üé§ TTS requests remaining: {remaining_tts}/10")

                        text_to_speech(prompt, GEMINI_API_KEY, path_audio)
                        result.path_audio = f"{path_audio}.wav"
                        print(f"‚úÖ Ho√†n th√†nh OCR + TTS cho ·∫£nh {id}")
                    else:
                        print(
                            f"‚ö†Ô∏è Kh√¥ng c√≥ GEMINI_API_KEY ƒë·ªÉ t·∫°o TTS cho {id}")
                except Exception as e:
                    print(f"‚ùå L·ªói t·∫°o TTS cho {id}: {str(e)}")
                    result.path_audio = ""
            elif result.items:
                print(f"‚è≠Ô∏è B·ªè qua TTS cho ·∫£nh {id} - kh√¥ng c√≥ text h·ª£p l·ªá")
        else:
            if result.items and has_valid_text(result.items):
                path_audio = f"public/tts/{result.id}.wav"
                text_to_speech_v2(normalize(result.items), path_audio)
                result.path_audio = path_audio
                result.has_bubble = True

        return result
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω ·∫£nh {id}: {str(e)}")
        return OcrResponse(id=id, items=[], path_audio="")


def process_single_image(image_url: str, id: str) -> OcrResponse:

    temp_path = f"temp_image_{id}.jpg"
    try:
        resp = requests.get(image_url)
        resp.raise_for_status()
        with open(temp_path, 'wb') as f:
            f.write(resp.content)

        _, height, size_bytes = get_image_size(temp_path)
        ocr_model = "ocrspace"
        if height > 1500 and size_bytes >= 1024 * 1024:
            ocr_model = "easyocr"

        ocr_results = extract(temp_path, ocr=ocr_model)

        items: list[OcrItem] = []
        for result in ocr_results:
            text_box = result.get("text_box", [])
            if len(text_box) >= 4:
                min_y, min_x, max_y, max_x = map(int, text_box[:4])
                # X√°c ƒë·ªãnh lo·∫°i bubble
                bubble_type = TypeBubble.DIALOGUE
                t = result.get("type", "").lower()
                if t == "thought":
                    bubble_type = TypeBubble.THOUGHT
                elif t == "narration":
                    bubble_type = TypeBubble.NARRATION
                elif t == "sound_effect":
                    bubble_type = TypeBubble.SOUND_EFFECT
                elif t == "background":
                    bubble_type = TypeBubble.BACKGROUND

                items.append(
                    OcrItem(
                        min_y=min_y, min_x=min_x,
                        max_y=max_y, max_x=max_x,
                        text=result.get("text"),
                        panel_id=result.get("panel_id"),
                        type=bubble_type,
                        model_used=ocr_model
                    )
                )

        return OcrResponse(id=id, items=items, path_audio="")
    except requests.RequestException as e:
        raise RuntimeError(f"L·ªói khi t·∫£i ·∫£nh: {str(e)}")
    except Exception as e:
        raise RuntimeError(f"L·ªói x·ª≠ l√Ω OCR: {str(e)}")
    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)


def normalize(ocr_items: List[OcrItem]) -> str:
    formatted_content = ""
    current_panel = None

    for item in ocr_items:
        if not item.text or not item.text.strip():
            continue

        # Th√™m separator gi·ªØa c√°c panel
        if current_panel != item.panel_id:
            if current_panel is not None:
                formatted_content += "\n\n"

        # Ch·ªâ l·∫•y n·ªôi dung text thu·∫ßn t√∫y, kh√¥ng th√™m prefix
        text = item.text.strip()
        formatted_content += f"{text}\n"

    return formatted_content


def create_tts_prompt(ocr_items: List[OcrItem]) -> str:
    """
    T·∫°o prompt TTS t·ª´ danh s√°ch OCR items

    Args:
        ocr_items: Danh s√°ch c√°c item OCR
        character_name: T√™n nh√¢n v·∫≠t ch√≠nh
        story_context: B·ªëi c·∫£nh c√¢u chuy·ªán

    Returns:
        str: Prompt ƒë√£ ƒë∆∞·ª£c format cho TTS
    """
    formatted_content = normalize(ocr_items)
    # T·∫°o prompt cu·ªëi c√πng
    prompt = tts_prompt_template.format(
        formatted_content=formatted_content
    )

    return prompt


if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
